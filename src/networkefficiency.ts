/* NIST disclaimer
==============================================================================*/

import * as nn from "./nn";
import * as playground from "./playground";
import {Example2D} from "./dataset";


/**
 * This class is for computing network inefficiency using KL divergence of pairs of histograms
 * the histograms are created at each layer from the occurrence of possible outputs generated by all nodes in one layer
 * there is one histogram per layer and per output label (-1 or 1)
 * the KL divergence of data-driven histograms (probabilities) is computed against reference probabilities assumed to be
 * uniform but scaled based on ratios of training data point per label over all training data points
 *
 * @author Peter Bajcsy
 */
export class AppendingNetworkEfficiency {
  private netEfficiency: number[];
  private avgKLdivergence: number;
  private mapGlobal = null;

  constructor() {
    this.reset();
  }

  reset() {
      this.mapGlobal = [];
      this.netEfficiency = [];
      this.avgKLdivergence = -1;
  }

  public getMapGlobal():any[]{
    return this.mapGlobal;
  }
  public getNetEfficiency():number[]{
    return this.netEfficiency;
  }
  public getAvgKLdivergence():number{
    return this.avgKLdivergence;
  }
  /**
   * This method compute the inefficiency coefficient of each network layer
   * @param network
   */
  public getNetworkInefficiencyPerLayer(network: nn.Node[][],trainData:Example2D[], numEvalSamples:number): number[] {
    //return array
    //let netEfficiency: number[] = [];

    /* configPts contains sequences of 0 and 1 (one per layer) that
    * correspond to each node output being 0 or 1 depending on the input point
    * mapGlobal contains the histogram of those sequences over all points per network layer  */
    //let mapGlobal = [];
    for (let idx = 0; idx < network.length - 1; idx++)
      this.mapGlobal[idx] = new Map<string, number>();

    let configPts;
    // fins stats of imbalanced data
    let countNOne: number = 0; //count minus one labeled training data points
    let countPOne: number = 0; //count one labeled training data points
    trainData.forEach((point, i) => {
      let input = playground.constructInput(point.x, point.y);
      console.log('point:'+i +' val:' + input.toString() + ', label:' + point.label);
      // compute the output configuration at each layer per point
      configPts = nn.forwardNetEval(network, input);
      let output = nn.forwardProp(network, input);
      // assign hard label based on the output probability
      let label: string;
      if (output <= 0) {
        label = 'N';
      } else {
        label = 'P';
      }
      // count the ground truth labels
      if (point.label <= 0) {
        countNOne++;
      } else {
        countPOne++;
      }
      //console.log('configPts:'+configPts.toString() + ', prob label:' + output.toString() + ', resulting label:' + label.toString());

      for (let layerIdx = 1; layerIdx < network.length; layerIdx++) {
        let temp = label + '-' + configPts[layerIdx - 1]; // configuration string
        let flag = false;
        if (this.mapGlobal[layerIdx - 1].size > 0) {
          if ((this.mapGlobal[layerIdx - 1]).get(temp) > 0) {
            flag = true;
            //console.log('match: ' + temp + ', stored:' + mapGlobal[layerIdx-1].get(temp));
          }
        }
        if (flag) {
          this.mapGlobal[layerIdx - 1].set(temp, this.mapGlobal[layerIdx - 1].get(temp) + 1);
        } else {
          this.mapGlobal[layerIdx - 1].set(temp, 1);
        }
      }
    });

    //sanity check
    if(countNOne <= 0 || countPOne <= 0){
      console.log('ERROR: training data contains only one label countNOne:' + countNOne + ', countPOne:' + countPOne);
      let element = document.getElementById("KLdivergenceDiv");
      element.innerHTML = 'ERROR: training data contains only one label countNOne:' + countNOne + ', countPOne:' + countPOne;
      return null;
    }
/*    // compute the network efficiency per layer
    let numSamples: number = (state.problem === Problem.REGRESSION) ?
        NUM_SAMPLES_REGRESS : NUM_SAMPLES_CLASSIFY;
    let numEvalSamples: number = numSamples * state.percTrainData / 100;*/

    this.avgKLdivergence = 0.0;  // this is to compute avg network KL divergence

    for (let layerIdx = 0; layerIdx < network.length - 1; layerIdx++) {
      let currentLayerNodeCount = network[layerIdx + 1].length;
      // the number of 0 or 1 sequence outcomes from a layer with currentLayerNodeCount nodes is
      // equal to 2^(currentLayerNodeCount ) .
      let numBins: number = Math.pow(2, currentLayerNodeCount);
      //let maxEntropy: number  = Math.log2(numBins);
      //console.log('maxEntropy for numBins:' + numBins + ' and currentLayerNodeCount:' + currentLayerNodeCount + ' is ' + maxEntropy);

      // define p_i for imbalanced classes
      // This number is multiplied by 2 since the outcomes are associated with
      // one of the two possible class labels (or  numBins corresponds to only one possible outcome)
      let refProb_NOne: number = 2 * (countNOne / numEvalSamples) * (1 / numBins);
      let refProb_POne: number = 2 * (countPOne / numEvalSamples) * (1 / numBins);
      console.log('countNOne:' + countNOne + ', countPOne:' + countPOne + ', refProb_NOne:' + refProb_NOne + ', refProb_POne:' + refProb_POne);
      //sanity check
      if (refProb_NOne <= 0 || refProb_POne <= 0) {
        console.log('ERROR: training data contains highly imbalanced labels refProb_NOne:' + refProb_NOne + ', refProb_POne:' + refProb_POne);
        this.netEfficiency[layerIdx] = 0;
        let element = document.getElementById("KLdivergenceDiv");
        element.innerHTML = 'ERROR: training data contains highly imbalanced labels refProb_NOne:' + refProb_NOne + ', refProb_POne:' + refProb_POne;
        return null;
      }
      // might be removed
      /*
      let samplesPerBin: number = numEvalSamples/numBins;
      console.log('num eval samples:' + numEvalSamples + ', expected number of samples per bin:' + samplesPerBin);
      // sanity check
      if (samplesPerBin < 1) {
        console.log('WARNING: there are more node outcomes (bins) than samples for numBins:' + numBins + ', numSamples:' + numSamples);
        samplesPerBin = 1.0;
      }
    */
      this.netEfficiency[layerIdx] = 0;
      this.mapGlobal[layerIdx].forEach((value: number, key: string) => {
        let prob = value / numEvalSamples;
        //console.log('inside:' + key, value, prob);
        if (key.substr(0, 1) === 'N') {
          this.netEfficiency[layerIdx] = this.netEfficiency[layerIdx] + prob * Math.log2(prob / refProb_NOne);
        } else {
          this.netEfficiency[layerIdx] = this.netEfficiency[layerIdx] + prob * Math.log2(prob / refProb_POne);
        }
      });
      //console.log('before final: layer:' + (layerIdx) + ', netEfficiency:' + netEfficiency[layerIdx]);
      //netEfficiency[layerIdx] = numBins * maxEntropy + netEfficiency[layerIdx];// -maxEntropy - netEfficiency[layerIdx]/numBins;
      // sanity check
      if (this.netEfficiency[layerIdx] < 0) {
        console.log('ERROR: layer:' + (layerIdx) + ', netEfficiency:' + this.netEfficiency[layerIdx] + ' is less than zero');
        this.netEfficiency[layerIdx] = 0;
      }
      console.log('layer:' + (layerIdx) + ', netEfficiency:' + this.netEfficiency[layerIdx]);
      this.avgKLdivergence = this.avgKLdivergence + this.netEfficiency[layerIdx];
    }

    this.avgKLdivergence = this.avgKLdivergence / this.netEfficiency.length;
    console.log('avg network efficiency:' + (Math.round(this.avgKLdivergence * 1000) / 1000).toString());

    //////////////////////////////////////////////////////////////
/*    // print the histograms and create histogram visualization
    let hist = new AppendingHistogramChart(this.mapGlobal, this.netEfficiency);
    //hist.createHistogramInputs(mapGlobal,netEfficiency);
    let kl_metric_result: string = hist.showKLHistogram();

    kl_metric_result += '&nbsp; avg KL value:' + (Math.round(avgKLdivergence * 1000) / 1000).toString() + '<BR>';
    let element = document.getElementById("KLdivergenceDiv");
    element.innerHTML = kl_metric_result;*/

    return this.netEfficiency;
  }



}
